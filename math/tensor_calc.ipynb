{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習で出る内積計算の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1次元 & 1次元\n",
      "tensor(-0.2639)\n",
      "------\n",
      "2次元 & 2次元\n",
      "torch.Size([3, 9])\n",
      "------\n",
      "2次元 & 1次元\n",
      "tensor([-2.3500, -3.8287,  2.5689]) torch.Size([3])\n",
      "------\n",
      "3次元 & 3次元\n",
      "torch.Size([10, 3, 9])\n",
      "------\n",
      "tensor(-2.1956)\n",
      "torch.Size([3])\n",
      "3次元 & 3次元:  torch.Size([10, 3, 9])\n",
      "3次元 & 2次元:  torch.Size([10, 3, 5])\n",
      "torch.Size([10, 2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 1次元 & 1次元\n",
    "a   = torch.randn([4])\n",
    "b   = torch.randn([4])\n",
    "c   = torch.dot(a, b)\n",
    "print('1次元 & 1次元')\n",
    "print(c)\n",
    "print('------')\n",
    "\n",
    "# 2次元 & 2次元\n",
    "a   = torch.randn([3, 4])\n",
    "b   = torch.randn([4, 9])\n",
    "c   = torch.mm(a, b)\n",
    "print('2次元 & 2次元')\n",
    "print(c.shape)\n",
    "print('------')\n",
    "\n",
    "# 2次元 & 1次元\n",
    "a   = torch.randn([3, 4])\n",
    "b   = torch.randn([4])\n",
    "c   = torch.mv(a, b)\n",
    "print('2次元 & 1次元')\n",
    "print(c, c.shape)\n",
    "print('------')\n",
    "\n",
    "# 3次元 & 3次元 (batch 入り2次元と考える)\n",
    "# 2次元 & 1次元\n",
    "a   = torch.randn([10, 3, 4])\n",
    "b   = torch.randn([10, 4, 9])\n",
    "c   = torch.bmm(a, b)\n",
    "print('3次元 & 3次元')\n",
    "print(c.shape)\n",
    "print('------')\n",
    "\n",
    "# 一般: matmul 使用\n",
    "## 1次元 & 1次元\n",
    "a   = torch.randn([4])\n",
    "b   = torch.randn([4])\n",
    "c   = torch.matmul(a, b)\n",
    "print(c)\n",
    "\n",
    "## 2次元 & 1次元\n",
    "a   = torch.randn([3, 4])\n",
    "b   = torch.randn([4])\n",
    "c   = torch.matmul(a, b)\n",
    "print(c.shape)\n",
    "\n",
    "## 3次元 & 3次元\n",
    "a   = torch.randn([10, 3, 4])\n",
    "b   = torch.randn([10, 4, 9])\n",
    "c   = torch.matmul(a, b)\n",
    "print('3次元 & 3次元: ', c.shape)\n",
    "\n",
    "## 3次元 & 2次元\n",
    "a   = torch.randn([10, 3, 4])\n",
    "b   = torch.randn([4, 5])\n",
    "c   = torch.matmul(a, b)\n",
    "print('3次元 & 2次元: ', c.shape)\n",
    "\n",
    "##  4次元 & 3次元\n",
    "a   = torch.randn([10, 2, 3, 4])\n",
    "b   = torch.randn([2, 4, 5])\n",
    "c   = torch.matmul(a, b)\n",
    "print(c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アダマール積の例\n",
    "\n",
    "### 目標\n",
    "Jin-Hwa et al. (2018) \"Bilinear Attention Networks\" の (10)式のlogits項\n",
    "$$\n",
    "    A_{i, j} = ((\\mathbb{1}\\cdot{\\boldsymbol p}^{T})\\circ{\\boldsymbol X}^{T}{\\boldsymbol U}){\\boldsymbol V}^{T}{\\boldsymbol Y}\n",
    "$$\n",
    "を計算したい。ここで、$\\mathbb{1} \\in \\mathbb{R}^{\\rho}, {\\boldsymbol p} \\in \\mathbb{R}^{K},  {\\boldsymbol X} \\in \\mathbb{R}^{N\\times \\rho},  {\\boldsymbol Y} \\in \\mathbb{R}^{M\\times \\phi}, {\\boldsymbol U} \\in \\mathbb{R}^{N\\times K}, {\\boldsymbol V} \\in \\mathbb{R}^{M\\times K}$であるので、結局 $${\\boldsymbol A} \\in \\mathbb{R}^{\\rho \\times \\phi} $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1= tensor([[ 2,  6],\n",
      "        [12, 20]])\n",
      "c2= tensor([[ 2,  6],\n",
      "        [12, 20]])\n",
      "a= tensor([[[1, 2, 3, 4],\n",
      "         [4, 3, 2, 1],\n",
      "         [2, 3, 4, 5]],\n",
      "\n",
      "        [[2, 3, 4, 5],\n",
      "         [5, 4, 3, 2],\n",
      "         [3, 4, 5, 6]]])\n",
      "b= tensor([[[1, 2, 3, 4],\n",
      "         [4, 3, 2, 1],\n",
      "         [2, 3, 4, 5]],\n",
      "\n",
      "        [[2, 3, 4, 5],\n",
      "         [5, 4, 3, 2],\n",
      "         [3, 4, 5, 6]]])\n",
      "c= tensor([[[ 1,  4,  9, 16],\n",
      "         [16,  9,  4,  1],\n",
      "         [ 4,  9, 16, 25]],\n",
      "\n",
      "        [[ 4,  9, 16, 25],\n",
      "         [25, 16,  9,  4],\n",
      "         [ 9, 16, 25, 36]]])\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.]],\n",
      "\n",
      "        [[2., 3., 4.],\n",
      "         [2., 3., 4.],\n",
      "         [2., 3., 4.],\n",
      "         [2., 3., 4.]]])\n",
      "A_size:  torch.Size([2, 4, 5])\n",
      "A:  tensor([[[ 36.,  50.,  64.,  78.,  92.],\n",
      "         [ 50.,  70.,  90., 110., 130.],\n",
      "         [ 64.,  90., 116., 142., 168.],\n",
      "         [ 78., 110., 142., 174., 206.]],\n",
      "\n",
      "        [[ 50.,  70.,  90., 110., 130.],\n",
      "         [ 70.,  99., 128., 157., 186.],\n",
      "         [ 90., 128., 166., 204., 242.],\n",
      "         [110., 157., 204., 251., 298.]]])\n",
      "A2_size:  torch.Size([2, 4, 5])\n",
      "A2:  tensor([[[ 36.,  50.,  64.,  78.,  92.],\n",
      "         [ 50.,  70.,  90., 110., 130.],\n",
      "         [ 64.,  90., 116., 142., 168.],\n",
      "         [ 78., 110., 142., 174., 206.]],\n",
      "\n",
      "        [[ 50.,  70.,  90., 110., 130.],\n",
      "         [ 70.,  99., 128., 157., 186.],\n",
      "         [ 90., 128., 166., 204., 242.],\n",
      "         [110., 157., 204., 251., 298.]]])\n"
     ]
    }
   ],
   "source": [
    "# 2次元 x 2次元のアダマール積\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[2,3],[4,5]])\n",
    "c1 = a*b   # * による演算\n",
    "print('c1=', c1)\n",
    "c2 = torch.einsum('ij,ij->ij', a, b)   # einsum を使う\n",
    "print('c2=', c2)\n",
    "\n",
    "# 3次元 x 3次元で最初の1次元がバッチの次元で残りがアダマール積\n",
    "a = torch.tensor([[[1, 2, 3, 4], [4, 3, 2, 1], [2, 3, 4, 5]], [[2, 3, 4 ,5], [5, 4 , 3, 2], [3, 4, 5, 6]]])\n",
    "b = torch.tensor([[[1, 2, 3, 4], [4, 3, 2, 1], [2, 3, 4, 5]], [[2, 3, 4 ,5], [5, 4 , 3, 2], [3, 4, 5, 6]]])\n",
    "c = torch.einsum('ijk,ijk->ijk', a, b)\n",
    "print('a=', a)\n",
    "print('b=', b)\n",
    "print('c=', c)\n",
    "print(c.shape)\n",
    " \n",
    "# (9)式で batch ありを考える。X'=U^{T}X, Y'=V^{T}Y がすでに計算済みとする。X': (b_size, rho, K), Y': (b_size, K, phi) の次元\n",
    "# batch_size: 2次元, K: 3次元, rho: : 4, phi: 5 と仮定すると\n",
    "uni = torch.Tensor([[1, 1, 1, 1], [1, 1, 1, 1]])\n",
    "p   = torch.Tensor([[1, 2, 3], [2, 3, 4]])\n",
    "x_ = torch.Tensor([[[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]], [[10, 20, 30], [20, 30, 40], [30, 40, 50], [40, 50, 60]]])\n",
    "y_ = torch.Tensor([[[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]], [[.1, .2, .3, .4, .5], [.2, .3, .4, .5, .6], [.3, .4, .5, .6, .7]]])\n",
    "\n",
    "p_ = torch.einsum('bi,bj->bij', uni, p)\n",
    "print(p_.shape)\n",
    "print(p_)\n",
    "\n",
    "A = torch.einsum('brk,brk,bkp->brp', p_, x_, y_)\n",
    "print('A_size: ', A.shape)\n",
    "print('A: ', A)\n",
    "\n",
    "# p_ を計算しなくても、einsumの機能で p_ の計算をスキップできる\n",
    "p__ = p.unsqueeze(0)\n",
    "A2 = torch.einsum('xrk,brk,bkp->brp', p_, x_, y_)\n",
    "print('A2_size: ', A.shape)\n",
    "print('A2: ', A)\n",
    "\n",
    "\n",
    "# A = torch.einsum('', uni, p, )\n",
    "# さらに、glimpse の次元まで考えると...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]]) torch.Size([3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,1,1])\n",
    "b = torch.Tensor([1,2,3,4])\n",
    "c = torch.einsum('i,j->ij', a, b)  # ある意味broadcast\n",
    "print(c, c.shape)\n",
    "\n",
    "c[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  6.,   9.,  12.,  15.],\n",
      "        [ 60.,  90., 120., 150.]])\n",
      "tensor([[  6.,   9.,  12.,  15.],\n",
      "        [ 60.,  90., 120., 150.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]], [[10, 20, 30], [20, 30, 40], [30, 40, 50], [40, 50, 60]]])\n",
    "torch.sum(x, 2).shape\n",
    "\n",
    "print(torch.sum(x, 2))\n",
    "print(x.sum(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
